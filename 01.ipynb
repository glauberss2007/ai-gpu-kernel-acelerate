{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "156a7283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting triton\n",
      "  Using cached triton-3.3.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n",
      "Collecting setuptools>=40.8.0\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Installing collected packages: setuptools, triton\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 80.9.0\n",
      "    Uninstalling setuptools-80.9.0:\n",
      "      Successfully uninstalled setuptools-80.9.0\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.3.1\n",
      "    Uninstalling triton-3.3.1:\n",
      "      Successfully uninstalled triton-3.3.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.7.0+cu118 requires triton==3.3.0; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have triton 3.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed setuptools-80.9.0 triton-3.3.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install triton --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5c47025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "import torch  # Biblioteca para computação tensorial e redes neurais\n",
    "import triton  # Biblioteca para otimização de operações em GPUs\n",
    "import triton.language as tl  # Módulo de linguagem específico do Triton para kernels\n",
    "from triton.runtime import driver  # Módulo para interagir com o driver do Triton\n",
    "import os  # Módulo para interagir com o sistema operacional\n",
    "\n",
    "# Função para calcular o softmax de forma ingênua (sem otimização)\n",
    "def naive_softmax(x):\n",
    "    # Encontra o valor máximo ao longo das linhas (dim=1)\n",
    "    x_max = x.max(dim=1)[0]\n",
    "    \n",
    "    # Subtrai o valor máximo de cada elemento para estabilidade numérica\n",
    "    z = x - x_max[:, None]\n",
    "    \n",
    "    # Calcula o exponencial de cada elemento\n",
    "    numerator = torch.exp(z)\n",
    "    \n",
    "    # Soma os exponenciais ao longo das linhas para obter o denominador\n",
    "    denominator = numerator.sum(dim=1)\n",
    "    \n",
    "    # Retorna o softmax: exponencial dividido pela soma dos exponenciais\n",
    "    return numerator / denominator[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46ffd996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorador para indicar que a função é um kernel Triton JIT (Just-In-Time)\n",
    "@triton.jit\n",
    "def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr, num_stages: tl.constexpr):\n",
    "    # Obtém o índice inicial da linha para o programa atual (thread block)\n",
    "    row_start = tl.program_id(0)\n",
    "    # Obtém o número de passos (steps) para processar as linhas\n",
    "    row_step = tl.num_programs(0)\n",
    "    \n",
    "    # Loop para processar as linhas em paralelo\n",
    "    for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):\n",
    "        # Calcula o ponteiro inicial da linha atual na matriz de entrada\n",
    "        row_start_ptr = input_ptr + row_idx * input_row_stride\n",
    "        # Cria offsets para as colunas dentro do bloco\n",
    "        col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "        # Calcula os ponteiros para os elementos da linha atual\n",
    "        input_ptrs = row_start_ptr + col_offsets\n",
    "        # Cria uma máscara para evitar acessos fora dos limites da matriz\n",
    "        mask = col_offsets < n_cols\n",
    "        # Carrega os elementos da linha atual, usando a máscara e substituindo valores inválidos por -inf\n",
    "        row = tl.load(input_ptrs, mask=mask, other=-float('inf'))\n",
    "        \n",
    "        # Subtrai o valor máximo da linha para estabilidade numérica\n",
    "        row_minus_max = row - tl.max(row, axis=0)\n",
    "        # Calcula o exponencial dos valores ajustados\n",
    "        numerator = tl.exp(row_minus_max)\n",
    "        # Calcula a soma dos exponenciais para normalização\n",
    "        denominator = tl.sum(numerator, axis=0)\n",
    "        # Calcula o softmax: exponencial dividido pela soma dos exponenciais\n",
    "        softmax_output = numerator / denominator\n",
    "        \n",
    "        # Calcula o ponteiro inicial da linha atual na matriz de saída\n",
    "        output_row_start_ptr = output_ptr + row_idx * output_row_stride\n",
    "        # Calcula os ponteiros para os elementos da linha de saída\n",
    "        output_ptrs = output_row_start_ptr + col_offsets\n",
    "        # Armazena o resultado do softmax na matriz de saída, usando a máscara\n",
    "        tl.store(output_ptrs, softmax_output, mask=mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffb7393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o dispositivo como a primeira GPU disponível\n",
    "DEVICE = torch.device(\"cuda:0\")\n",
    "\n",
    "# Obtém as propriedades da GPU ativa\n",
    "properties = driver.active.utils.get_device_properties(DEVICE.index)\n",
    "NUM_SM = properties[\"multiprocessor_count\"]  # Número de multiprocessadores (SMs)\n",
    "NUM_REGS = properties[\"max_num_regs\"]  # Número máximo de registradores por SM\n",
    "SIZE_SMEM = properties[\"max_shared_mem\"]  # Tamanho máximo da memória compartilhada por SM\n",
    "WARP_SIZE = properties[\"warpSize\"]  # Tamanho de um warp (32 threads)\n",
    "target = triton.runtime.driver.active.get_current_target()  # Alvo de compilação atual\n",
    "\n",
    "# Função para calcular o softmax usando o kernel Triton\n",
    "def softmax(x):\n",
    "    # Obtém o número de linhas e colunas da matriz de entrada\n",
    "    n_rows, n_cols = x.shape\n",
    "    # Define o tamanho do bloco como a próxima potência de 2 maior que o número de colunas\n",
    "    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n",
    "    num_warps = 8  # Número de warps por bloco\n",
    "    num_stages = 4  # Número de estágios para o pipeline de execução\n",
    "    # Cria uma matriz de saída vazia com o mesmo formato da entrada\n",
    "    y = torch.empty_like(x)\n",
    "    \n",
    "    # Pré-aquecimento do kernel (compilação e inicialização)\n",
    "    kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE, num_stages=num_stages, num_warps=num_warps, grid=(1,))\n",
    "    kernel._init_handles()  # Inicializa os handles do kernel\n",
    "    n_regs = kernel.n_regs  # Número de registradores usados pelo kernel\n",
    "    size_smem = kernel.metadata.shared  # Tamanho da memória compartilhada usada pelo kernel\n",
    "    \n",
    "    # Calcula a ocupação do kernel (quantos blocos podem ser executados por SM)\n",
    "    occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)\n",
    "    occupancy = min(occupancy, SIZE_SMEM // size_smem)\n",
    "    \n",
    "    # Define o número de programas (blocos) a serem executados\n",
    "    num_programs = NUM_SM * occupancy\n",
    "    num_programs = min(num_programs, n_rows)  # Limita ao número de linhas\n",
    "    \n",
    "    # Executa o kernel com os parâmetros calculados\n",
    "    kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)\n",
    "    \n",
    "    # Imprime o código IR (Intermediate Representation) do kernel\n",
    "    print(kernel.asm['ttir'])\n",
    "    # Outras opções para depuração (comentadas):\n",
    "    # print(kernel.asm['ttgir'])\n",
    "    # print(triton_kernel.asm['llir'])\n",
    "    # print(triton_kernel.asm['ptx'])\n",
    "    # print(triton_kernel.asm['cubin'])\n",
    "    \n",
    "    return y  # Retorna a matriz de saída após o cálculo do softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4681817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#loc = loc(\"/tmp/ipykernel_1349/820465314.py\":3:0)\n",
      "#loc1 = loc(unknown)\n",
      "#loc12 = loc(\"/tmp/ipykernel_1349/820465314.py\":23:37)\n",
      "#loc17 = loc(\"/tmp/ipykernel_1349/820465314.py\":27:29)\n",
      "#loc26 = loc(callsite(#loc1 at #loc12))\n",
      "#loc29 = loc(callsite(#loc1 at #loc17))\n",
      "module {\n",
      "  tt.func public @softmax_kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc(\"/tmp/ipykernel_1349/820465314.py\":3:0), %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc(\"/tmp/ipykernel_1349/820465314.py\":3:0), %arg2: i32 loc(\"/tmp/ipykernel_1349/820465314.py\":3:0), %arg3: i32 loc(\"/tmp/ipykernel_1349/820465314.py\":3:0), %arg4: i32 loc(\"/tmp/ipykernel_1349/820465314.py\":3:0), %arg5: i32 loc(\"/tmp/ipykernel_1349/820465314.py\":3:0)) attributes {noinline = false} {\n",
      "    %cst = arith.constant dense<0xFF800000> : tensor<1024xf32> loc(#loc1)\n",
      "    %0 = tt.get_program_id x : i32 loc(#loc2)\n",
      "    %1 = tt.get_num_programs x : i32 loc(#loc3)\n",
      "    scf.for %arg6 = %0 to %arg4 step %1  : i32 {\n",
      "      %2 = arith.muli %arg6, %arg2 : i32 loc(#loc5)\n",
      "      %3 = tt.addptr %arg1, %2 : !tt.ptr<f32>, i32 loc(#loc6)\n",
      "      %4 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32> loc(#loc7)\n",
      "      %5 = tt.splat %3 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>> loc(#loc8)\n",
      "      %6 = tt.addptr %5, %4 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32> loc(#loc8)\n",
      "      %7 = tt.splat %arg5 : i32 -> tensor<1024xi32> loc(#loc9)\n",
      "      %8 = arith.cmpi slt, %4, %7 : tensor<1024xi32> loc(#loc9)\n",
      "      %9 = tt.load %6, %8, %cst : tensor<1024x!tt.ptr<f32>> loc(#loc10)\n",
      "      %10 = \"tt.reduce\"(%9) <{axis = 0 : i32}> ({\n",
      "      ^bb0(%arg7: f32 loc(callsite(#loc1 at #loc12)), %arg8: f32 loc(callsite(#loc1 at #loc12))):\n",
      "        %21 = arith.maxnumf %arg7, %arg8 : f32 loc(#loc31)\n",
      "        tt.reduce.return %21 : f32 loc(#loc25)\n",
      "      }) : (tensor<1024xf32>) -> f32 loc(#loc25)\n",
      "      %11 = tt.splat %10 : f32 -> tensor<1024xf32> loc(#loc14)\n",
      "      %12 = arith.subf %9, %11 : tensor<1024xf32> loc(#loc14)\n",
      "      %13 = math.exp %12 : tensor<1024xf32> loc(#loc15)\n",
      "      %14 = \"tt.reduce\"(%13) <{axis = 0 : i32}> ({\n",
      "      ^bb0(%arg7: f32 loc(callsite(#loc1 at #loc17)), %arg8: f32 loc(callsite(#loc1 at #loc17))):\n",
      "        %21 = arith.addf %arg7, %arg8 : f32 loc(#loc32)\n",
      "        tt.reduce.return %21 : f32 loc(#loc28)\n",
      "      }) : (tensor<1024xf32>) -> f32 loc(#loc28)\n",
      "      %15 = tt.splat %14 : f32 -> tensor<1024xf32> loc(#loc19)\n",
      "      %16 = arith.divf %13, %15 : tensor<1024xf32> loc(#loc19)\n",
      "      %17 = arith.muli %arg6, %arg3 : i32 loc(#loc20)\n",
      "      %18 = tt.addptr %arg0, %17 : !tt.ptr<f32>, i32 loc(#loc21)\n",
      "      %19 = tt.splat %18 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>> loc(#loc22)\n",
      "      %20 = tt.addptr %19, %4 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32> loc(#loc22)\n",
      "      tt.store %20, %16, %8 : tensor<1024x!tt.ptr<f32>> loc(#loc23)\n",
      "    } {tt.num_stages = 4 : i32} loc(#loc4)\n",
      "    tt.return loc(#loc24)\n",
      "  } loc(#loc)\n",
      "} loc(#loc)\n",
      "#loc2 = loc(\"/tmp/ipykernel_1349/820465314.py\":5:30)\n",
      "#loc3 = loc(\"/tmp/ipykernel_1349/820465314.py\":7:31)\n",
      "#loc4 = loc(\"/tmp/ipykernel_1349/820465314.py\":10:57)\n",
      "#loc5 = loc(\"/tmp/ipykernel_1349/820465314.py\":12:46)\n",
      "#loc6 = loc(\"/tmp/ipykernel_1349/820465314.py\":12:36)\n",
      "#loc7 = loc(\"/tmp/ipykernel_1349/820465314.py\":14:35)\n",
      "#loc8 = loc(\"/tmp/ipykernel_1349/820465314.py\":16:37)\n",
      "#loc9 = loc(\"/tmp/ipykernel_1349/820465314.py\":18:29)\n",
      "#loc10 = loc(\"/tmp/ipykernel_1349/820465314.py\":20:22)\n",
      "#loc11 = loc(\"/home/glauber/ai-gpu-kernel-acelerate/gpu-env/lib/python3.10/site-packages/triton/language/standard.py\":184:40)\n",
      "#loc13 = loc(\"/home/glauber/ai-gpu-kernel-acelerate/gpu-env/lib/python3.10/site-packages/triton/language/standard.py\":163:27)\n",
      "#loc14 = loc(\"/tmp/ipykernel_1349/820465314.py\":23:30)\n",
      "#loc15 = loc(\"/tmp/ipykernel_1349/820465314.py\":25:27)\n",
      "#loc16 = loc(\"/home/glauber/ai-gpu-kernel-acelerate/gpu-env/lib/python3.10/site-packages/triton/language/standard.py\":286:36)\n",
      "#loc18 = loc(\"/home/glauber/ai-gpu-kernel-acelerate/gpu-env/lib/python3.10/site-packages/triton/language/standard.py\":256:15)\n",
      "#loc19 = loc(\"/tmp/ipykernel_1349/820465314.py\":29:37)\n",
      "#loc20 = loc(\"/tmp/ipykernel_1349/820465314.py\":32:54)\n",
      "#loc21 = loc(\"/tmp/ipykernel_1349/820465314.py\":32:44)\n",
      "#loc22 = loc(\"/tmp/ipykernel_1349/820465314.py\":34:45)\n",
      "#loc23 = loc(\"/tmp/ipykernel_1349/820465314.py\":36:30)\n",
      "#loc24 = loc(\"/tmp/ipykernel_1349/820465314.py\":10:4)\n",
      "#loc25 = loc(callsite(#loc11 at #loc12))\n",
      "#loc27 = loc(callsite(#loc13 at #loc11))\n",
      "#loc28 = loc(callsite(#loc16 at #loc17))\n",
      "#loc30 = loc(callsite(#loc18 at #loc16))\n",
      "#loc31 = loc(callsite(#loc27 at #loc12))\n",
      "#loc32 = loc(callsite(#loc30 at #loc17))\n",
      "\n",
      "Triton time: 0.1059565544128418\n",
      "Torch time: 0.0009646415710449219\n",
      "tensor([[3.2747e-04, 5.3954e-04, 5.8687e-05,  ..., 5.5490e-04, 2.0330e-04,\n",
      "         9.4091e-04],\n",
      "        [8.6914e-04, 1.1075e-03, 1.0202e-04,  ..., 1.1470e-04, 5.5436e-04,\n",
      "         8.6046e-04],\n",
      "        [1.4101e-03, 1.0511e-03, 1.7031e-03,  ..., 1.2272e-03, 7.9059e-04,\n",
      "         3.2380e-04],\n",
      "        ...,\n",
      "        [7.9013e-04, 2.2518e-03, 1.5464e-04,  ..., 2.9425e-03, 3.1403e-04,\n",
      "         3.8015e-04],\n",
      "        [1.4756e-03, 2.1638e-03, 7.7872e-04,  ..., 4.2272e-03, 3.1717e-04,\n",
      "         2.3309e-04],\n",
      "        [2.5341e-04, 2.8272e-04, 4.0345e-04,  ..., 1.3202e-03, 1.4116e-03,\n",
      "         5.9517e-04]], device='cuda:0')\n",
      "tensor([[3.2747e-04, 5.3954e-04, 5.8687e-05,  ..., 5.5490e-04, 2.0330e-04,\n",
      "         9.4091e-04],\n",
      "        [8.6914e-04, 1.1075e-03, 1.0202e-04,  ..., 1.1470e-04, 5.5436e-04,\n",
      "         8.6046e-04],\n",
      "        [1.4101e-03, 1.0511e-03, 1.7031e-03,  ..., 1.2272e-03, 7.9059e-04,\n",
      "         3.2380e-04],\n",
      "        ...,\n",
      "        [7.9013e-04, 2.2518e-03, 1.5464e-04,  ..., 2.9425e-03, 3.1403e-04,\n",
      "         3.8015e-04],\n",
      "        [1.4756e-03, 2.1638e-03, 7.7872e-04,  ..., 4.2272e-03, 3.1717e-04,\n",
      "         2.3309e-04],\n",
      "        [2.5341e-04, 2.8272e-04, 4.0345e-04,  ..., 1.3202e-03, 1.4116e-03,\n",
      "         5.9517e-04]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Importa a biblioteca time para medir o tempo de execução\n",
    "import time\n",
    "\n",
    "# Define uma semente para garantir reprodutibilidade nos resultados\n",
    "torch.manual_seed(0)\n",
    "# Cria uma matriz aleatória de tamanho 1823x781 na GPU\n",
    "x = torch.randn(1823, 781, device=DEVICE)\n",
    "\n",
    "# Mede o tempo de execução da implementação do softmax usando Triton\n",
    "start_time = time.time()\n",
    "y_triton = softmax(x)  # Executa a função softmax implementada com Triton\n",
    "print(\"Triton time:\", time.time() - start_time)  # Imprime o tempo gasto\n",
    "\n",
    "# Mede o tempo de execução da implementação ingênua do softmax usando PyTorch\n",
    "start_time = time.time()\n",
    "y_torch = naive_softmax(x)  # Executa a função softmax ingênua\n",
    "print(\"Torch time:\", time.time() - start_time)  # Imprime o tempo gasto\n",
    "\n",
    "# Imprime os resultados das duas implementações\n",
    "print(y_triton)  # Resultado do Triton\n",
    "print(y_torch)  # Resultado do PyTorch\n",
    "\n",
    "# Verifica se os resultados das duas implementações são próximos (comentado para evitar erro se houver diferenças mínimas)\n",
    "assert torch.allclose(y_triton, y_torch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
